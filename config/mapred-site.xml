<?xml version='1.0'?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <!-- These properties are generated by ClusterConsole automatically --> 
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>master1:10020</value>
        <description>Internal RPC address of the job history server</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>master1:19888</value>
        <description>External Web Address to reach the job history server</description>
    </property>

    <!-- End of auto-generated properties -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <!-- Default number of Map and Reduce tasks to 2 for hbase clusters -->
    <property>
        <name>mapreduce.tasktracker.map.tasks.maximum</name>
        <value>2</value>
        <description>The maximum number of map tasks that will be run simultaneously by a task tracker.</description>
    </property>   
    <property>
        <name>mapreduce.tasktracker.reduce.tasks.maximum</name>
        <value>2</value>
        <description>The maximum number of reduce tasks that will be run simultaneously by a task tracker.</description>
    </property>   

    <!-- This property is overriden in gridforce-cluster profile. Default is 5 -->
    <property>
        <name>mapreduce.job.end-notification.retry.attempts</name>
        <value>5</value>
    </property>

    <property>
        <name>mapreduce.job.end-notification.max.attempts</name>
        <value>5</value>
    </property>

    <property>
    	<name>mapreduce.shuffle.port</name>
    	<value>8082</value>
    	<description>Default port that the ShuffleHandler will run on. We use 8082 to avoid conflict with the core app, which runs on 8080, and the monitoring app, which runs on 8081. In dev mode this causes issues. ShuffleHandler is a service run at the NodeManager to facilitate transfers of intermediate Map outputs to requesting Reducers.</description>
    </property>
    <property>
        <name>mapreduce.task.io.sort.mb</name>
        <value>256</value>
        <description>Cloudera recommneds:512 if higher memory-limit while sorting data for efficiency.</description>
    </property>
    <property>
        <name>mapreduce.task.io.sort.factor</name>
        <value>100</value>
        <description>Defaults to 10. The number of streams to merge at once while while sorting files.</description>
    </property>
    <property>
        <name>mapreduce.reduce.shuffle.parallelcopies</name>
        <value>30</value>
        <description>Cloudera recommends:50 Higher number of parallel copies run by reduces to fetch outputs from very large number of maps.</description>
    </property>
    <property>
        <name>mapreduce.job.reduce.slowstart.completedmaps</name>
        <value>0.95</value>
        <description>Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job. Defailuts to 0.05. At this setting, reducers will be held up for a longer duration.</description>
    </property>
    <property>
        <name>mapreduce.reduce.shuffle.merge.percent</name>
        <value>0.66</value>
        <description>As is Default:The usage threshold at which an in-memory merge will be
        initiated, expressed as a percentage of the total memory allocated to
        storing in-memory map outputs, as defined by
        mapreduce.reduce.shuffle.input.buffer.percent.
        </description>
    </property>
    <property>
        <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
        <value>0.6</value>
        <description>The percentage of memory to be allocated from the maximum heap
        size to storing map outputs during the shuffle.
        </description>
    </property>
    <property>
        <name>mapreduce.cluster.administrators</name>
        <value>hdfs</value>
    </property>
    <property>
        <name>mapreduce.reduce.input.buffer.percent</name>
        <value>0.0</value>
        <description>The percentage of memory- relative to the maximum heap size- to
        retain map outputs during the reduce. When the shuffle is concluded, any
        remaining map outputs in memory must consume less than this threshold
        before the reduce can begin.
        </description>
    </property>
    <property>
        <name>mapreduce.map.output.compress</name>
        <value>true</value>
    </property>
    <property>
        <name>mapreduce.map.output.compress.codec</name>
        <value>org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    <property>
        <name>mapreduce.output.fileoutputformat.compress.type</name>
        <value>BLOCK</value>
    </property>
    <property>
        <name>mapreduce.reduce.speculative</name>
        <value>false</value>
    </property>
    <property>
        <name>mapreduce.map.speculative</name>
        <value>false</value>
    </property>
    <property>
        <name>mapreduce.task.timeout</name>
        <value>600000</value>
        <description>The number of milliseconds before a task will be
        terminated if it neither reads an input, writes an output, nor
        updates its status string.
        </description>
    </property>
    <property>
        <name>stream.tmpdir</name>
        <value/>
    </property>
    <property>
        <name>mapred.child.java.opts</name>
        <value>-server -Xmx4096m -Djava.net.preferIPv4Stack=true -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/sfdc/logs/hbase-hadoop/@taskid@</value>
        <description>No description</description>
    </property>
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>5120</value>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>5120</value>
    </property>
    <property>
        <name>mapreduce.admin.map.child.java.opts</name>
        <value>-XX:NewRatio=8 -Djava.net.preferIPv4Stack=true</value>
        <final>true</final>
    </property>
    <property>
        <name>mapreduce.admin.reduce.child.java.opts</name>
        <value>-XX:NewRatio=8 -Djava.net.preferIPv4Stack=true</value>
        <final>true</final>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>1536</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.command-opts</name>
        <value>-Xmx1024m -XX:NewRatio=8 -Djava.net.preferIPv4Stack=true</value>
        <description>Java opts for the MR App Master processes.
        The following symbol, if present, will be interpolated: @taskid@ is replaced
        by current TaskID. Any other occurrences of '@' will go unchanged.
        For example, to enable verbose gc logging to a file named for the taskid in
        /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of:
        -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc

        Usage of -Djava.library.path can cause programs to no longer function if
        hadoop native libraries are used. These values should instead be set as part
        of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and
        mapreduce.reduce.env config settings.
        </description>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.admin-command-opts</name>
        <value>-XX:NewRatio=8 -Djava.net.preferIPv4Stack=true</value>
        <description>Java opts for the MR App Master processes for admin purposes.
        It will appears before the opts set by yarn.app.mapreduce.am.command-opts and
        thus its options can be overridden user. 

        Usage of -Djava.library.path can cause programs to no longer function if
        hadoop native libraries are used. These values should instead be set as part 
        of LD_LIBRARY_PATH in the map / reduce JVM env using the mapreduce.map.env and 
        mapreduce.reduce.env config settings. 

        Currently a copy of am.command-opts until release that supports admin opts
        is deployed!
        </description>
    </property>
    <property>
        <name>mapreduce.job.hdfs-servers</name>
        <value>${fs.defaultFS}</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.job.client.port-range</name>
        <value>50500-51000</value>
        <final>true</final>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.job.task.listener.thread-count</name>
        <value>30</value>
        <description>The number of threads used to handle RPC calls in the
        MR AppMaster from remote tasks</description>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms</name>
        <value>1000</value>
        <description>The interval in ms at which the MR AppMaster should send
        heartbeats to the ResourceManager</description>
    </property>
    <property>
        <name>yarn.app.mapreduce.client-am.ipc.max-retries</name>
        <value>1</value>
        <description>The number of client retries to the AM - before reconnecting
        to the RM to fetch Application Status.</description>
    </property>
    <property>
        <name>yarn.app.mapreduce.client.max-retries</name>
        <value>3</value>
        <description>The number of client retries to the RM/HS/AM before
        throwing exception. This is a layer above the ipc.</description>
    </property>
    <property>
        <name>mapreduce.jobhistory.client.thread-count</name>
        <value>10</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.joblist.cache.size</name>
        <value>20000</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.loadedjobs.cache.size</name>
        <value>40</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.max-age-ms</name>
        <value>604800000</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.move.interval-ms</name>
        <value>180000</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.move.thread-count</name>
        <value>3</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.cleaner.interval-ms</name>
        <value>86400000</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.intermediate-done-dir</name>
        <value>/mapred/history/done_intermediate</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/mapred/history/done</value>
    </property>
<!-- [Dheeren] Recheck when enabling security 
    <property>
        <name>mapreduce.jobhistory.keytab</name>
        <value>/etc/grid-keytabs/cobaltblue-jt1.prod.service.keytab</value>
        <description></description>
    </property>
    <property>
        <name>mapreduce.jobhistory.principal</name>
        <value>mapred/</value>
        <description></description>
    </property>
-->
    <property>
        <name>yarn.app.mapreduce.am.create-intermediate-jh-base-dir</name>
        <value>false</value>
    </property>
    <property>
        <name>mapreduce.map.maxattempts</name>
        <value>4</value>
    </property>
    <property>
        <name>mapreduce.reduce.maxattempts</name>
        <value>4</value>
    </property>
    <property>
        <name>mapreduce.job.maxtaskfailures.per.tracker</name>
        <value>3</value>
    </property>
    <property>
        <name>mapreduce.client.submit.file.replication</name>
        <value>5</value>
    </property>
</configuration>
